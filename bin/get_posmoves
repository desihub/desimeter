#!/usr/bin/env python

import os
import psycopg2
import numpy as np
from astropy.table import Table, vstack, Column
import argparse
import time
start_time = time.time()

from desimeter.util import parse_fibers
from desimeter.dbutil import dbquery,get_petal_ids,get_pos_ids,get_petal_loc

enable_keys_moves = {'CTRL_ENABLED'}
enable_keys_calib = {'DEVICE_CLASSIFIED_NONFUNCTIONAL', 'FIBER_INTACT'}

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter,
                                     description="""Retrieve data from posmove DB and save to disk as CSV table""")

parser.add_argument('--host', type= str, required=False, default='db.replicator.dev-cattle.stable.spin.nersc.org', help="db.replicator.dev-cattle.stable.spin.nersc.org for kpno or beyonce.lbl.gov for petaltest")
parser.add_argument('--port', type= int, required=False, default=60042, help="60042 for kpno or 5432 for petaltest")
parser.add_argument('--password', type= str, required=False, default=None, help="nothing for kpno")
parser.add_argument('--petal-ids', type= str, required=False, default=None, help="comma separated list of petal ids")
parser.add_argument('--exposure-ids', type= str, required=False, default=None, help="comma separated list of exposure ids")
parser.add_argument('--exposure-iters', type= str, required=False, default=None, help="comma separated list of exposure iters")
default_min_date = '2019-01-01'
default_max_date = '2050-01-01'
parser.add_argument('--date-min', type = str, default = default_min_date, required = False, help="date min with format YYYY-MM-DD")
parser.add_argument('--date-max', type = str, default = default_max_date, required = False, help="date max with format YYYY-MM-DD")
parser.add_argument('--pos-ids', type= str, required=False, default=None, help="comma separated list of positioner ids (same as DEVICE_ID). alternately, argue 'movers' to only pull data files for those robots with move data")
parser.add_argument('-o', '--outdir', type = str, default = "None", required = True, help="output directory where MXXXX.csv files are saved")
parser.add_argument('-c', '--with-calib', action = 'store_true', help="add matching calib from db")
parser.add_argument('-t', '--tp-updates', action='store_true', help="include rows where no move was done but POS_T, POS_P was updated")
parser.add_argument('--recent-rehome-exposure-ids', type= str, required=False, default=None, help="comma separated list of exposure ids where the positioners have been recently rehomed")
parser.add_argument('-comma', '--comma-replacement', type=str, default='||', help='replace commas in data strings with this in output csv files)')
parser.add_argument('-e', '--enabled-history', action='store_true', help=f'filter the data down to just those rows in which a change occurred to one of {enable_keys_moves | enable_keys_calib}')
parser.add_argument('-m', '--merge', action='store_true', help='merge all data into one CSV file (rather than separate files per positioner)')
parser.add_argument('-l', '--latest', type=int, default=0, help="Gets N latest data rows for each positioner. Use of this option overrides several other options, including: exposure-ids, exposure-iters, date-min, date-max, with-calib, tp-updates, recent-rehome-exposure-ids, enabled-history. Default 0 means 'all'.")

args  = parser.parse_args()

# example: get_posmoves --host beyonce.lbl.gov --port 5432 --password XXXX --petal-ids 1 --exposure-ids 2107 --exposure-iters 0,1,2,3,4,5,6,7,8,9,10,11 --outdir tmp --nocalib --pos-id M02155

# open connection
comm = psycopg2.connect(host=args.host,port=args.port, database='desi_dev', user='desi_reader',password=args.password)

order_by = 'order by time_recorded desc' # canonically order things by descending when fetching from DB, affects interpretation of "latest" below
if args.latest:
    assert args.latest > 0, f"can't get {args.latest} rows!"
    args.exposure_ids = None 
    args.exposure_iters = None
    args.date_min = default_min_date 
    args.date_max = default_max_date
    args.with_calib = True
    args.tp_updates = True
    args.recent_rehome_exposure_ids = None 
    args.enabled_history = False
    order_by += f' limit {args.latest}'

if args.petal_ids is not None :
    petalids = parse_fibers(args.petal_ids)
else :
    petalids = get_petal_ids(comm)

recent_rehome_exposure_ids=list()
if args.recent_rehome_exposure_ids is not None :
    recent_rehome_exposure_ids=[int(val) for val in args.recent_rehome_exposure_ids.split(",")]

if args.enabled_history:
    enable_keys = enable_keys_moves
    if args.with_calib:
        enable_keys |= enable_keys_calib
    desired_fields = {'moves': 'time_recorded, pos_id, petal_id, device_loc, bus_id, ctrl_enabled, move_cmd, move_val1, move_val2, log_note, exposure_id, exposure_iter, flags',
                      'calib': 'time_recorded, device_classified_nonfunctional, fiber_intact, calib_note'}
else:
    desired_fields = {'moves': '*', 'calib': '*'}

if args.pos_ids is not None :
    only_posids = args.pos_ids.split(",")
else :
    only_posids = None

otables = []

i_ptl = 0
for petalid in petalids :
    i_ptl += 1


    # read posids from db, for a given petal
    from_where = f'from posmovedb.positioner_moves_p{int(petalid)} where'
    posid_date = "time_recorded BETWEEN date '{}' and date '{}'".format(args.date_min,args.date_max)
    cmd = f'select distinct pos_id {from_where} {posid_date}'
    if args.exposure_ids is not None:
        cmd += " and exposure_id in ({})".format(args.exposure_ids)
    if args.exposure_iters is not None:
        cmd += " and exposure_iter in ({})".format(args.exposure_iters)
    res=dbquery(comm, cmd)

    posids=res["pos_id"]

    if only_posids is not None :
        posids = np.intersect1d(posids,only_posids)

    if len(posids)==0 : continue

    petal_loc = get_petal_loc(petalid)

    i_pos = 0
    for posid in posids :
        i_pos += 1

        # read full data from db, for a given positioner
        from_where = f'from posmovedb.positioner_moves_p{int(petalid)} where'
        posid_date = "pos_id='{}' and time_recorded BETWEEN date '{}' and date '{}'".format(posid,args.date_min,args.date_max)
        cmd_exp = ''
        if args.exposure_ids is not None:
            cmd_exp += "exposure_id in ({})".format(args.exposure_ids)
        if args.exposure_iters is not None:
            cmd_exp += ' and ' if cmd_exp else ''
            cmd_exp += "exposure_iter in ({})".format(args.exposure_iters)
        cmd_tp = ''
        if args.tp_updates:
            cmd_idx = f'select pos_move_index {from_where} {posid_date}'
            if cmd_exp:
                cmd_idx += f' and {cmd_exp}'
            cmd += order_by 
            result = dbquery(comm, cmd_idx)
            if any(result['pos_move_index']):
                min_idx = min(result['pos_move_index'])
                max_idx = max(result['pos_move_index'])
                max_idx += 1  # capture any tp_updates from just after the exposure or exp iteration
                cmd_tp = f"log_note like '%tp_update%' and pos_move_index >= {min_idx} and pos_move_index <= {max_idx}"
        cmd = f'select {desired_fields["moves"]} {from_where} {posid_date}'
        if cmd_exp and cmd_tp:
            combined = f'({cmd_exp}) or ({cmd_tp})'
            cmd += f' and ({combined})'
        elif cmd_exp:
            cmd += f' and ({cmd_exp})'
        cmd += order_by 
        posmoves=dbquery(comm,cmd)

        if args.pos_ids == 'movers':
            has_move_cmd = {x not in {'', None} for x in posmoves['move_cmd']}
            if not any(has_move_cmd):
                continue

        # add petal loc
        posmoves["PETAL_LOC"] = np.repeat(petal_loc,len(posmoves["petal_id"]))

        # make sure there is no comma in log_note
        for i in range(len(posmoves["log_note"])) :
            if posmoves["log_note"][i] is not None:
                posmoves["log_note"][i]=str(posmoves["log_note"][i]).replace(",", args.comma_replacement)

        if args.with_calib :

            # adding calib info
            cmd = f'select {desired_fields["calib"]} from posmovedb.positioner_calibration_p{petalid} where pos_id=\'{posid}\''
            cmd += order_by 
            calib=dbquery(comm,cmd)

            # get time stamps to match
            tstamp_calib     =  np.array([d.timestamp() for d in calib["time_recorded"]])
            tstamp_posmovedb =  np.array([d.timestamp() for d in posmoves["time_recorded"]])

            rename_keys={'time_recorded':'calib_time_recorded'}

            new_keys=list()
            for k in calib.keys() :
                if k in rename_keys.keys() :
                    new_keys.append(rename_keys[k])
                else :
                    new_keys.append(k)

            for k in new_keys :
                posmoves[k]=list()

            for t in tstamp_posmovedb :
                j=np.where(tstamp_calib<=t)[0][-1]
                for k1,k2 in zip(new_keys,calib.keys()) :
                    posmoves[k1].append(calib[k2][j])

        # save, using astropy.Table io

        # by default convert keys to upper case,
        # but with special cases (of course),
        rename_keys={
            'obs_x':'X_FP',
            'obs_y':'Y_FP',
            'time_recorded':'DATE',
            'calib_time_recorded':'CALIB_DATE',
        }

        otable = Table()
        for k in posmoves.keys() :

            if k in rename_keys.keys() :
                k2=rename_keys[k]
            else :
                k2=k.upper()

            otable[k2]=np.array(posmoves[k])

        otable["RECENT_REHOME"]=np.in1d(otable["EXPOSURE_ID"],recent_rehome_exposure_ids).astype(int)

        if args.enabled_history:
            selection = np.array([False]*len(otable))
            selection[0] = True  # by default, include the first row
            otable.sort('DATE')
            for key in enable_keys:
                selection[1:] |= otable[key][1:] != otable[key][:-1]  # detect changes
            otable = otable[selection]

        # remove non measured entries
        if not args.tp_updates:
            ok=(otable["X_FP"] != None)
            otable=otable[ok]

        if not os.path.isdir(args.outdir):
            os.makedirs(args.outdir)

        status_str = f'positioner {i_pos} of {len(posids)} on petal {i_ptl} of {len(petalids)}'
        if args.merge:
            otables.append(otable)
            print(f'retrieved data for {posid}, {status_str}')
        else:
            ofilename="{}/{}.csv".format(args.outdir,posid)
            otable.write(ofilename,overwrite=True)
            print("wrote", ofilename, status_str)
if args.merge:
    empty = [i for i in range(len(otables)) if len(otables[i]) == 0]
    for i in reversed(empty):
        del otables[i]
    existing_keys = set(otables[0].columns)  # presume all tables have same keys at this point
    for key in existing_keys:
        for table in otables:
            table[key] = Column(table[key].tolist(), dtype=object)
    print(f'merging {len(otables)} tables...')
    merged = vstack(otables)
    merged.sort(['PETAL_ID', 'POS_ID'])
    name = time.strftime('%Y%m%dT%H%M%S%z') + '_posdata.csv'
    path = os.path.join(args.outdir, name)
    merged.write(path)
    print('merge complete')
    print(f'saved to {path}')
print(f'Total elapsed time: {time.time() - start_time:.1f} sec')
