#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analysis script comparing internally-tracked (theta, phi) to fvc-measured (x,y).
The goal is to examine the general performance of all positioners during any
move (not just a specialized calibration sequence).

Data sources:

    theta and phi:
    --------------
    The positioner moves database contains time-stamped data for values POS_T
    and POS_P. These correspond to focalplane coordinates posintTP.

    x and y:
    --------
    The NERSC and KPNO servers contain numerous FITS files. These are processed
    by desimeter. A result X_FP, Y_FP is generated from the centroids. This
    corresponds to focalplane coordinates ptlXY.

    As of 2020-04-08, J. Guy identified 9472 such files from the commissioning
    operations so far at the Mayall, and has run them through desimeter. See
    Julien's email to desi-commiss@desi.lbl.gov of 2020-04-08, in which he
    attaches several descriptive slides.

    The fvc catalogs he produced are on NERSC at:
        /global/cfs/cdirs/desi/engineering/fvc/desimeter/v0.2.0/

    And the matched coordinates per positioner are on NERSC at:
        /global/cfs/cdirs/desi/engineering/fvc/desimeter/v0.2.0/positioners-3/
"""

# command line argument parsing
import argparse
description = 'Fits fiber positioner calibration parameters. Fits are ' + \
              'performed by comparing measured  (x,y) to internally-' + \
              'tracked (theta,phi).'
parser = argparse.ArgumentParser(description=description)
parser.add_argument('-i', '--infiles', type=str, required=True, nargs='*',
                    help='path(s) to input csv file(s). Regex ok (like M*.csv). ' + \
                         'Multiple file args also ok (like M0001.csv M0002.csv M01*.csv)')
parser.add_argument('-o', '--outdir', type=str, required=True,
                    help='path to directory where to save output files')
parser.add_argument('-dw', '--data_window', type=int, default=100,
                    help='minimum width (num pts) for window swept through historical data')
parser.add_argument('-pd', '--period_days', type=int, default=1,
                    help='spacing of datum dates at which to run the fit')
parser.add_argument('-np', '--n_processes_max', type=int, default=None,
                    help='max number of processors to use. Note: can argue 1 to avoid multiprocessing pool (useful for debugging)')
parser.add_argument('-sq', '--static_quantile', type=float, default=0.05,
                    help='sort static param fits by fit error, take this fraction with least error, and median them to decide the single set of best values')
parser.add_argument('-v', '--verbose', action='store_true',
                    help='extra verbosity of print outs at terminal')
parser.add_argument('--log-note-selection', type=str, default=None, required=False,
                    help="required keywords in LOG_NOTE selection, seperated by '&', like ''arc calibration & use_desimeter=True'")
parser.add_argument('--no-circle-fit', action='store_true', help="do not initiate the fit with a fit of theta circles")

args = parser.parse_args()

# proceed with the rest
import os
import sys
import multiprocessing
from pkg_resources import resource_filename
import glob

# imports below require <path to desimeter>/py' to be added to system PYTHONPATH.
import desimeter.posparams.fithandler as fithandler

# paths
desimeter_data_dir = resource_filename("desimeter", "data")
infiles = []
for s in args.infiles:
    these = glob.glob(s)
    infiles.extend(these)
infiles = [os.path.realpath(p) for p in infiles]
if not os.path.isdir(args.outdir):
    os.path.os.makedirs(args.outdir)

# parse out the posids from filenames
csv_files = {}
posid_str_length = 6
for file in infiles:
    basename = os.path.basename(file)
    posid, ext = os.path.splitext(basename)
    posid = posid[:posid_str_length]
    if  ext == '.csv' and len(posid) == posid_str_length and posid[0].isalpha() and posid[1:].isnumeric():
        path = os.path.realpath(file)
        csv_files[posid] = path
if not csv_files:
    print('No recognizable csv files found for analysis.')
    sys.exit()

# verbosity control
if args.verbose:
    printf = print
else:
    def printf(*args, **kwargs):
        pass

if __name__ == '__main__':
    print(f'Now processing {len(csv_files)} csv data files...')
    with multiprocessing.Pool(processes=args.n_processes_max) as pool:
        for posid, path in csv_files.items():
            kwargs = {'posid': posid,
                      'path': path,
                      'period_days': args.period_days,
                      'data_window': args.data_window,
                      'savedir': args.outdir,
                      'static_quantile': args.static_quantile,
                      'printf': printf,
                      'log_note_selection': args.log_note_selection,
                      'no_circle_fit': args.no_circle_fit
                      }
            if args.n_processes_max == 1:
                logstr = fithandler.run_best_fits(**kwargs)
                print(logstr)
            else:
                pool.apply_async(fithandler.run_best_fits,
                                 kwds=kwargs,
                                 callback=print)
        pool.close()
        pool.join()
    print('Complete.')
