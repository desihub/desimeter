#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Tool for analyzing positioner performance. Typically operates on data tables
retrieved using the "get_posmoves --with-calib" tool.
"""

# command line argument parsing
import argparse
parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument('-i', '--infiles', type=str, required=True, nargs='*',
                    help='Path to input csv file(s), containing results from "get_posmoves --with-calib" ' +
                         '(see that function''s help for full syntax). Regex is ok (like M*.csv). Multiple ' +
                         'file args are also ok (like M00001.csv M00002.csv M01*.csv), as is a directory ' +
                         'that contains the files.')
parser.add_argument('-o', '--outdir', type=str, required=True, help='Path to directory where to save output files.')
parser.add_argument('-np', '--n_processes_max', type=int, default=None,  help='max number of processors to use')
args = parser.parse_args()

# proceed with bulk of imports
import os
import glob
import multiprocessing
import numpy as np
import math
from astropy.table import Table, vstack
from astropy.time import Time
import desimeter.transform.pos2ptl as pos2ptl
import matplotlib.pyplot as plt

# paths
infiles = []
for s in args.infiles:
    these = glob.glob(s)
    for this in these:
        if os.path.isdir(this):
            contents = os.listdir(this)
            contents = [os.path.join(this, file) for file in contents]
            infiles.extend(contents)
        else:
            infiles.append(this)
infiles = [os.path.realpath(p) for p in infiles]
save_dir = os.path.realpath(args.outdir)
if not os.path.isdir(save_dir):
    os.path.os.makedirs(save_dir)

# data identifiers and keywords
required = {'PETAL_ID', 'POS_ID', 'POS_MOVE_INDEX', 'POS_T', 'POS_P', 'PTL_X', 'PTL_Y',
            'LENGTH_R1', 'LENGTH_R2', 'OFFSET_X', 'OFFSET_Y', 'OFFSET_T', 'OFFSET_P',
            'MOVE_CMD', 'EXPOSURE_ID', 'EXPOSURE_ITER', 'DEVICE_LOC', 'DATE', 'LOG_NOTE'}
sequence_note_prefix = 'sequence: '

# statistics functions for summarizing errors
stat_funcs = {'max': max, 'min': min, 'mean': np.mean, 'median': np.median,
              'std': np.std, 'rms': lambda X: (sum(x**2 for x in X)/len(X))**0.5}

# function defs
def nulls(*args):
    '''Returns set of indexes for null entries in one or more argued arrays.'''
    special_null_cases = {'None', 'none', '(null)', 'null'}
    idxs = set()
    for array in args:
        idxs |= {i for i in range(len(array)) if not(array[i]) or array[i] in special_null_cases}
    return idxs

def tokenize_note(log_note):
    '''Returns list of tokens in a LOG_NOTE entry.'''
    tokens = log_note.split(';')
    tokens = [token.strip() for token in tokens]
    return tokens

def parse_val(string, separator=' ', typefunc=int):
    '''Parse a number out of typical token format from LOG_NOTE strings.'''
    split = string.split(separator)
    val = typefunc(split[1]) if len(split) > 0 else None
    return val

def remove_nonmoving_rows(table):
    '''Deletes rows with no move.
    
    INPUTS:  table ... astropy table, as generated by read()
    OUTPUT:  new ... copy of table, with equal or fewer rows
    '''
    new = table.copy()
    null_check_cols = ['MOVE_CMD', 'PTL_X', 'PTL_Y', 'EXPOSURE_ID']
    null_check_arrays = [new[key].tolist() for key in null_check_cols]
    null_idxs = nulls(*null_check_arrays)
    exp_iters = new['EXPOSURE_ITER'].tolist()  # special handling because value of 0 is ok
    null_iters = {i for i in nulls(exp_iters) if exp_iters[i] != 0}
    null_idxs |= null_iters
    new.remove_rows(list(null_idxs))
    return new

def remove_unused_columns(table):
    '''Deletes unnecessary columns, for easier data viewing.
    
    INPUTS:  table ... astropy table, as generated by read()
    OUTPUT:  new ... copy of table, with equal or fewer columns
    '''
    new = table.copy()
    useless = set(new.columns) - required
    new.remove_columns(list(useless))
    return new

def read(path):
    '''Read in data from disk. If data has multiple posids, their respective data
    will be returned in multiple tables.
    
    Basic validation that the required data exists will be performed. This includes
    checking that the data includes actual moves. Thus the function is tolerant of
    data files for non-moving positioners --- they will be automatically skipped.
    
    INPUTS:  path ... must be a csv file, including move and calibration data
    OUTPUT:  data ... dict with keys = posids and values = astropy tables
    '''
    data = {}
    if 'csv' not in path:
        return data  # i.e. skip non-data files
    table = Table.read(path)
    missing = required - set(table.columns)
    if any(missing):
        print(f'Skipped data at {path} due to missing columns {missing}')
        return data
    initial_posids = set(table['POS_ID'])
    table = remove_unused_columns(table)
    table = remove_nonmoving_rows(table)
    posids = set(table['POS_ID'])
    posids_removed = initial_posids - posids
    if any(posids_removed):
        print(f'Skipped data for {posids_removed} which contained no moves in {path}')
    for posid in posids:
        selection = table['POS_ID'] == posid
        this = table[selection]
        data[posid] = this
        print(f'Read data for {posid} at {path}')
    return data

def identify_submoves(table):
    '''Searches LOG_NOTE field to identify submoves (i.e. correction moves).
    
    INPUTS:  table ... astropy table containing 'LOG_NOTE' and 'EXPOSURE_ID'
    OUTPUT:  copy of table, with new columns 'MOVE' and 'SUBMOVE', containing
             integer IDs identifying any moves and submoves.
             
    Note that 'MOVE' values may not be unique, if the dataset includes multiple
    tests. In such cases, uniqueness can be determined by inspecting also the
    EXPOSURE_ID field --- within which MOVE *will* be unique.
    
    Additionally, note that 'MOVE' integers may not correspond 1:1 with those
    given in the LOG_NOTE fields, again due to handling of uniqueness corner
    cases. However, 'SUBMOVE' ids *will* correspond exactly to those in LOG_NOTE.
    '''
    new = table.copy()
    new.sort('POS_MOVE_INDEX')
    move_counter = -1
    moves = []
    submoves = []
    last_expid = new['EXPOSURE_ID'][0]
    last_named_move = None  # different from move_counter ... this is from LOG_NOTE field
    for i in range(len(new)):
        expid = new['EXPOSURE_ID'][i]
        if expid != last_expid:
            move_counter = -1
            last_expid = expid
        note = new['LOG_NOTE'][i]
        if 'submove' not in note:
            move_counter += 1
            submove = 0
        else:
            tokens = tokenize_note(note)
            found_move, found_submove = False, False
            for token in tokens:
                if token.find('move') == 0:
                    named_move = parse_val(token)
                    found_move = True
                if token.find('submove') == 0:
                    submove = parse_val(token)
                    found_submove = True
            assert found_move and found_submove, f'move/submove parsing error of LOG_NOTE: {note}'
            if named_move != last_named_move:
                move_counter += 1
                last_named_move = named_move
        moves.append(move_counter)
        submoves.append(submove)
    new['MOVE'] = moves
    new['SUBMOVE'] = submoves
    return new

def calc_errors(table):
    '''Identifies measured and target values, then calculates errors.
    
    INPUTS:  table ... astropy table containing 'PTL_X', 'PTL_Y', 'POS_T', 'POS_P',
                       'LENGTH_R1', 'LENGTH_R2', 'OFFSET_T', 'OFFSET_P', 'OFFSET_X',
                       'OFFSET_Y', 'SUBMOVE', and 'POS_MOVE_INDEX'
    OUTPUT:  copy of table, where measured data columns will have _MEAS  appended to
             their names (for clarity) and new columns will be added with _TARG suffix,
             as well as new columns with ERR_ prefix
    '''
    new = table.copy()
    new.sort('POS_MOVE_INDEX')
    for key in ['PTL_X', 'PTL_Y', 'PTL_Z', 'OBS_X', 'OBS_Y']:
        if key in new.columns:
            new.rename_column(key, f'{key}_MEAS')
    meas_x, meas_y = new['PTL_X_MEAS'], new['PTL_Y_MEAS']
    targ_x, targ_y = pos2ptl.int2ptl(t_int=new['POS_T'], p_int=new['POS_P'],
                                     r1=new['LENGTH_R1'], r2=new['LENGTH_R1'],
                                     t_offset=new['OFFSET_T'], p_offset=new['OFFSET_P'],
                                     x_offset=new['OFFSET_X'], y_offset=new['OFFSET_Y'])
    is_submove0 = new['SUBMOVE'] == 0
    targ_xy = [targ_x, targ_y]
    for i in [0, 1]:
        last_targ0 = targ_xy[i][0]
        for j in range(len(targ_xy[i])):
            if is_submove0[j]:
                last_targ0 = targ_xy[i][j]
            else:
                targ_xy[i][j] = last_targ0
    new['PTL_X_TARG'] = targ_x
    new['PTL_Y_TARG'] = targ_y
    new['ERR_X'] = meas_x - targ_x
    new['ERR_Y'] = meas_y - targ_y
    new['ERR_XY'] = np.hypot(new['ERR_X'], new['ERR_Y'])
    return new

def analyze_one_pos(table):
    '''Analyze the data for one table, containing one positioner's data.
    
    INPUTS:  table ... astropy table, as generated by read()
    OUTPUT:  (new, stats, msg)
    
    "new" is an astropy table, containing data copied from "table", as well as some
    new columns like calculated error values.
    
    "stats" is another  astropy table, containing summary error statistics for
    each submove in "table".
    
    "msg" is a string suitable for printing at stdout, may be ''
    '''
    msg = ''
    new = table.copy()
    new.sort('POS_MOVE_INDEX')
    posids = set(new['POS_ID'])
    assert len(posids) == 1, f'error: input to analyze should be a table with only one posid, not {posids}'
    posid = posids.pop()
    new = identify_submoves(new)
    new = calc_errors(new)
    stats_dict = {name:[] for name in ['POS_ID', 'SUBMOVE'] + list(stat_funcs)}
    submoves = sorted(set(new['SUBMOVE']))
    for submove in submoves:
        stats_dict['POS_ID'].append(posid)
        stats_dict['SUBMOVE'].append(submove)
        selection = new['SUBMOVE'] == submove
        err_xy = new['ERR_XY'][selection].tolist()
        for name, func in stat_funcs.items():
            stats_dict[name].append(func(err_xy))
    stats = Table(stats_dict)
    return new, stats, msg

def summarize_stats(table, sigmas=[1,2,3,4,5,math.inf], stat_names=['max', 'min', 'median', 'rms']):
    '''Generate summary statistics for each submove, gathering positioners into
    error quantiles at the specified "sigma" levels.
    
    INPUTS:  table ... astropy table with data for one or many positioners. The table
                       must include columns for 'SUBMOVE', 'POS_ID', and 'ERR_XY'.
                       
             sigmas ... list of ints giving which sigma quantiles to gather the data
                        into. In other words, sigma=1 corresponds to 63.2% quantile,
                        sigma=2 corresponds to 86.5%, etc. A value math.inf or np.inf
                        acts like a "100%" quantile.
                        
             stat_names ... names of which stat_funcs results to perform the quantiles
                            on. Must be a subset of stat_funcs.
    
    OUTPUTS: new ... astropy table with columns 'SUBMOVE', 'SIGMA', 'QUANTILE', 'N_TARGETS'
                     and columns corresponding to the uppercased names of the argued
                     stats functions. Table includes metadata: n_positioners, n_targets
    '''
    missing_funcs = set(stat_names) - set(stat_funcs)
    assert not(any(missing_funcs)), f'undefined stat funcs {missing_funcs}'
    uppercase_names = {key: key.upper() for key in stat_names}
    submoves = sorted(set(table['SUBMOVE']))
    new_columns = ['SUBMOVE', 'SIGMA', 'QUANTILE', 'N_TARGETS'] + list(uppercase_names.values())
    new_dict = {key:[] for key in new_columns}
    quantiles = {s: 1 - math.exp(-s) for s in sigmas}
    for submove in submoves:
        selection = table['SUBMOVE'] == submove
        err = table['ERR_XY'][selection].tolist()
        err = np.sort(err)
        for sigma in sigmas:
            quantile = quantiles[sigma]
            cutoff = np.quantile(err, quantile, interpolation='midpoint')
            selection = err <= cutoff
            these_err = err[selection]
            new_dict['SUBMOVE'].append(submove)
            new_dict['SIGMA'].append(sigma)
            new_dict['QUANTILE'].append(quantile)
            new_dict['N_TARGETS'].append(len(these_err))
            for name in stat_names:
                func = stat_funcs[name]
                stat_result = func(these_err)
                key = uppercase_names[name]
                new_dict[key].append(stat_result)
    new = Table(new_dict)
    new.meta['n_positioners'] = len(set(table['POS_ID']))
    new.meta['n_targets'] = len([x for x in table['SUBMOVE'] == sorted(submoves)[0] if x])
    return new

def meta_str(table):
    '''Produces a multi-line string representation of meta data in table.
    '''
    alts = {'n_positioners': 'Num positioners',
            'n_targets': 'Total num targets'}
    s = ''
    for key, val in table.meta.items():
        name = alts[key] if key in alts else key
        s += f'{name}: {val}\n'
    return s

def summary_stats_str(table, submove='all'):
    '''Produces a multi-line string representation of summary statistics
    
    INPUT:  table ... astropy table as-generated by summary_stats
            submove ... 'all' or integer
    
    OUTPUT: string
    '''
    int_list = lambda key: sorted([int(x) if x != math.inf else x for x in set(table[key])])
    submoves = int_list('SUBMOVE')
    if submove != 'all':
        assert submove in submoves
        submoves = [submove]
    sigmas = int_list('SIGMA')
    sigma_associates_list = lambda key: [table[key][table['SIGMA'] == s][0] for s in sigmas]
    quantiles = sigma_associates_list('QUANTILE')
    sigma_n_targs = sigma_associates_list('N_TARGETS')
    stat_names = [key for key in table.columns if key.lower() in stat_funcs]
    tab = '  '
    space = ' '
    unit = 'um'
    def row_str(arr, width=10, scale=1, suffix=''):
        cells = []
        adj_width = width - len(suffix)
        for x in arr:
            if isinstance(x, (int, np.integer, str)):
                cells.append(format(x, f'>{adj_width}') + str(suffix))
            else:
                cells.append(format(x*scale, f'>{adj_width}.1f') + str(suffix))
        out = space.join(cells)
        out += '\n'
        return out
    s = ''
    left_cell = lambda string: f'{tab}{string:>13}:'
    s += left_cell('sigma') + row_str(sigmas)
    s += left_cell('% of targets') + row_str(quantiles, scale=100, suffix='%')
    s += left_cell('num targets') + row_str(sigma_n_targs)
    for submove in submoves:
        prefix = f'SUBMOVE {submove} '
        for name in stat_names:
            selection = table['SUBMOVE'] == submove
            subtable = table[selection]
            selections = [subtable['SIGMA'] == sig for sig in sigmas]
            row_data = [subtable[name][sel][0] for sel in selections]
            s += left_cell(prefix + name) + row_str(row_data, scale=1000)
            prefix = '... '
    row_widths = [len(x) for x in s.split('\n')]
    header = f'Positioning errors summary (units={unit}):'
    s = format(header, f'<{max(row_widths)}') + '\n' + s
    return s

def histogram(stats_table):
    pass

def init_plot(figsize=(8,6), dpi=150):
    '''Initialize a plot. Returns figure handle.'''
    plt.ioff()
    fig = plt.figure(figsize=figsize, dpi=dpi)
    plt.clf()
    return fig

def save_and_close_plot(fig, savepath):
    '''Save plot to disk and close. Argue the figure handle to be closed, and the
    path where to save the image. Extension determines image format.'''
    plt.savefig(savepath, bbox_inches='tight')
    plt.close(fig)
    
def get_span(table, prefix='PTL_X'):
    '''Returns max - min for all measured and target 'PTL_X' or 'PTL_Y' data in table.'''
    vec = table[f'{prefix}_TARG'].tolist() + table[f'{prefix}_MEAS'].tolist()
    return max(vec) - min(vec)

def bigchart(table, summary_stats=None, extra_note=''):
    '''Plot all targets and measurements for positioners on one big chart.
    
    INPUT:  table ... astropy table of targets and measured positons, as generated by analyze()
            stats ... optional astropy table of summary statistics, as generated by summary_stats()
            extra_note ... optional multi-line string to include in plot annotations
            
    OUTPUT: paths ... paths to output plot file(s)
    '''
    limits = None
    paths = []
    submoves = sorted(set(table['SUBMOVE']))    
    for submove in submoves:
        selection = table['SUBMOVE'] == submove
        selection = [selection[i] for i in range(len(table))]
        assert any(selection)
        p = table[selection]
        if submoves.index(submove) == 0:
            dpi = 200
            min_plot_dims = (10, 8)
            dots_per_mm = 12
            data_width_mm = [get_span(p, 'PTL_X'), get_span(p, 'PTL_Y')]
            plot_width_in = [mm * dots_per_mm / dpi for mm in data_width_mm]
            plot_width_in = [max(plot_width_in[i], min_plot_dims[i]) for i in [0,1]]
            plot_width_in[1] += 2
        fig = init_plot(figsize=tuple(plot_width_in), dpi=dpi)
        for suffix, desc in {'TARG':'target', 'MEAS':'measured'}.items():
            shape = 'o' if suffix == 'TARG' else '+'
            plt.plot(p[f'PTL_X_{suffix}'], p[f'PTL_Y_{suffix}'], shape, label=desc)
        posids = set(table['POS_ID'])
        for posid in posids:
            selection = p['POS_ID'] == posid
            label_x = np.mean(p['PTL_X_TARG'][selection])
            label_y = np.mean(p['PTL_Y_TARG'][selection])
            loc = p['DEVICE_LOC'][selection][0]
            label_text = f'{posid}\n({loc})'
            plt.annotate(label_text, xy=(label_x, label_y), xycoords='data', 
                         horizontalalignment='center', verticalalignment='top',
                         fontfamily='monospace', fontsize='small')
        plt.xlabel('PTL_X (mm)')
        plt.ylabel('PTL_Y (mm)')
        expids = sorted(set(p['EXPOSURE_ID']))
        expids_str = '-'.join([str(x) for x in expids])
        dates = {datetime.split(' ')[0] for datetime in p['DATE']}
        dates = sorted(dates)
        vert_space = ''.join(['\n']*5)
        note = vert_space
        note += f'Exposure ID(s): {expids}\n'
        note += f'Submove: {submove}\n'
        note += f'Date(s) measured: {dates}\n'
        note += f'Date analyzed: {Time.now().iso.split(" ")[0]}\n'
        note += meta_str(summary_stats)
        note += extra_note
        strip_final_return = lambda s: s[:-1] if s[-1] == '\n' else s
        note = strip_final_return(note)
        note_fontsize = 'small'
        plt.annotate(note, xy=(0.01, 0.0), xycoords='axes fraction',
                     fontfamily='monospace', fontsize=note_fontsize,
                     verticalalignment='top', horizontalalignment='left')
        if summary_stats:
            summary = vert_space
            summary += summary_stats_str(summary_stats, submove=submove)
            summary = strip_final_return(summary)
            plt.annotate(summary, xy=(0.99, 0.0), xycoords='axes fraction',
                         fontfamily='monospace', fontsize=note_fontsize,
                         verticalalignment='top', horizontalalignment='right')
        
        plt.legend(loc='upper left')
        plt.axis('equal')
        if limits:
            plt.xlim(limits['x'])
            plt.ylim(limits['y'])
        limits = {'x': plt.xlim(), 'y': plt.ylim()}
        fig.tight_layout()
        path = os.path.join(save_dir, f'expid_{expids_str}_submove_{submove}')
        save_and_close_plot(fig, path + '.png')
        paths.append(path)
    return paths

# read in the data files
input_data = {}
for path in infiles:
    this_data = read(path)
    input_data.update(this_data)
assert any(input_data), 'No data to analyze. Please check that input file args are correct.'

# containers / handlers for multiprocessed analysis data
results_list = []
stats_list = []
def store_result(result, stat, msg):
    if msg:
        print(msg)
    if result and stat:
        results_list.append(result)
        stats_list.append(stat)

single_process = args.n_processes_max == 1  # useful for debugging
if __name__ == '__main__':
    if single_process:
        for table in input_data.values():
            result = analyze_one_pos(table)
            store_result(*result)
    else:
        with multiprocessing.Pool(processes=args.n_processes_max) as pool:
            pool_results = pool.imap(analyze_one_pos, input_data.values())
            pool.close()
            pool.join()
        for result in pool_results:
            store_result(*result)
    results = vstack(results_list)
    stats = vstack(stats_list)
    if single_process:
        summary_stats = summarize_stats(results)
        paths = bigchart(results, summary_stats=summary_stats, extra_note='')
    else:
        with multiprocessing.Pool(processes=args.n_processes_max) as pool:
            pass